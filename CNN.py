# -*- coding: utf-8 -*-
"""Applied Data Science.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WyupsIaoXAW7GRrwpFKa0QiR499OUmYz
"""

# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow.keras import models, layers

# Helper libraries
import matplotlib.pyplot as plt
import os

"""Constants"""

IMAGE_WIDTH=4000
IMAGE_HEIGHT=2250
BATCH_SIZE=7
EPOCH = 15
CHANNELS = 3

"""Where am I?"""

os.listdir()

"""Import data, gives each set of healthy and sic trees a class label"""

dataset = tf.keras.preprocessing.image_dataset_from_directory(
    "drive/MyDrive/Colab Notebooks/Tree_disease/training",
    shuffle=True,
    image_size=(IMAGE_WIDTH,IMAGE_HEIGHT),
    batch_size = BATCH_SIZE
)

class_names = dataset.class_names
class_names

len(dataset)

from google.colab import drive
drive.mount('/content/drive')

"""Create Training Dataset

75% train
12.5% validate
12.5% test
"""

train_size = 0.8
len(dataset) * train_size

train_ds = dataset.take(6)

test_ds = dataset.skip(6)

val_size = 0.1
len(dataset)*val_size

val_ds = test_ds.take(1)

def get_training_partitions_tf(ds, train_split=0.75, val_split=0.125, test_split =0.125, shuffle=True, shuffle_size=10000):

  ds_size = len(ds)

  if shuffle:
    ds = ds.shuffle(shuffle_size, seed=12)

  train_size= int(train_split * ds_size)
  val_size= int(val_split * ds_size)

  train_ds = ds.take(train_size)
  val_ds = ds.skip(train_size).take(val_size)
  test_ds = ds.skip(train_size).skip(val_size)

  return train_ds, val_ds, test_ds

train_ds, val_ds, test_ds = get_training_partitions_tf(dataset)

"""Data Augmentation to use later, lets us create augmented images for more images and better fitment.

some problem with recognizing layers? something isnt imported correctly
"""

data_augmentation = tf.keras.Sequential([
    layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
    layers.experimental.preprocessing.RandomRotation(0.2)
])

"""conv2D(filter, kernal_size, activation) filter is subjective. needs to be tested and tweaked

activation is 'relu', just common and fast to compute

Amount of conv and pooling layers are subjective, NEEDS TESTING
"""

shape = (BATCH_SIZE, IMAGE_WIDTH, IMAGE_HEIGHT, CHANNELS)
n_classes = 3

model = models.Sequential([
    data_augmentation,
    layers.Conv2D(32, (2,2), activation = 'relu', input_shape = shape),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, (2,2), activation = 'relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, (2,2), activation = 'relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, (2,2), activation = 'relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, (2,2), activation = 'relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, (2,2), activation = 'relu'),
    layers.MaxPooling2D((2,2)),
    layers.Flatten(),
    layers.Dense(32, activation='relu'),
    layers.Dense(n_classes, activation='softmax')
])

model.build(input_shape= shape)

"""Smaller Kernal size below"""

model.summary()

model.compile(
    optimizer='adam',
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

model.fit(
    x = train_ds,
    epochs = EPOCH,
    batch_size=BATCH_SIZE,
    verbose=1,
    validation_data=val_ds
)